{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1a781b3-ac9a-43bd-97ff-bbb0ad6e5d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e485e76-bece-454d-82f1-e7b281a10b3e",
   "metadata": {},
   "source": [
    "# SBI Example with Permutation Invariant Density\n",
    "\n",
    "The code inspiration can be found [here](https://github.com/smsharma/jet-setting/blob/1c07c72f3354936093589f66547d2face89036f3/notebooks/01_jets_set_transformer.ipynb#L129)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8556a53-7a99-41b6-bdce-0a1c704aa9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stribor as st\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e0d6af-fccf-4670-89eb-7896d9f2456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flow(nn.Module):\n",
    "    \"\"\"\n",
    "    Building both normalizing flows and neural flows.\n",
    "\n",
    "    Example:\n",
    "    >>> import stribor as st\n",
    "    >>> torch.manual_seed(123)\n",
    "    >>> dim = 2\n",
    "    >>> flow = st.Flow(st.UnitNormal(dim), [st.Affine(dim)])\n",
    "    >>> x = torch.rand(1, dim)\n",
    "    >>> y, ljd = flow(x)\n",
    "    >>> y_inv, ljd_inv = flow.inverse(y)\n",
    "\n",
    "    Args:\n",
    "        base_dist (Type[torch.distributions]): Base distribution\n",
    "        transforms (List[st.flows]): List of invertible transformations\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dist=None, transforms=[]):\n",
    "        super().__init__()\n",
    "        self.base_dist = base_dist\n",
    "        self.transforms = nn.ModuleList(transforms)\n",
    "\n",
    "    def forward(self, x, latent=None, mask=None, t=None, reverse=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (tensor): Input sampled from base density with shape (..., dim)\n",
    "            latent (tensor, optional): Conditional vector with shape (..., latent_dim)\n",
    "                Default: None\n",
    "            mask (tensor): Masking tensor with shape (..., 1)\n",
    "                Default: None\n",
    "            t (tensor, optional): Flow time end point. Default: None\n",
    "            reverse (bool, optional): Whether to perform an inverse. Default: False\n",
    "\n",
    "        Returns:\n",
    "            y (tensor): Output that follows target density (..., dim)\n",
    "            log_jac_diag (tensor): Log-Jacobian diagonal (..., dim)\n",
    "        \"\"\"\n",
    "        transforms = self.transforms[::-1] if reverse else self.transforms\n",
    "        _mask = 1 if mask is None else mask\n",
    "\n",
    "        log_jac_diag = torch.zeros_like(x).to(x)\n",
    "        for f in transforms:\n",
    "            if reverse:\n",
    "                x, ld = f.inverse_and_log_det_jacobian(x * _mask, latent=latent, mask=mask, t=t, **kwargs)\n",
    "            else:\n",
    "                x, ld = f.forward_and_log_det_jacobian(x * _mask, latent=latent, mask=mask, t=t, **kwargs)\n",
    "            log_jac_diag += ld * _mask\n",
    "        return x, log_jac_diag\n",
    "\n",
    "    def inverse(self, y, latent=None, mask=None, t=None, **kwargs):\n",
    "        \"\"\" Inverse of forward function with the same arguments. \"\"\"\n",
    "        return self.forward(y, latent=latent, mask=mask, t=t, reverse=True, **kwargs)\n",
    "\n",
    "    def log_prob(self, x, **kwargs):\n",
    "        \"\"\"\n",
    "        Calculates log-probability of a sample.\n",
    "\n",
    "        Args:\n",
    "            x (tensor): Input with shape (..., dim)\n",
    "\n",
    "        Returns:\n",
    "            log_prob (tensor): Log-probability of the input with shape (..., 1)\n",
    "        \"\"\"\n",
    "        if self.base_dist is None:\n",
    "            raise ValueError('Please define `base_dist` if you need log-probability')\n",
    "        x, log_jac_diag = self.inverse(x, **kwargs)\n",
    "\n",
    "        log_prob = self.base_dist.log_prob(x) + log_jac_diag.sum(-1)\n",
    "        return log_prob.unsqueeze(-1)\n",
    "\n",
    "    def sample(self, num_samples, latent=None, mask=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Transforms samples from the base to the target distribution.\n",
    "        Uses reparametrization trick.\n",
    "\n",
    "        Args:\n",
    "            num_samples (tuple or int): Shape of samples\n",
    "            latent (tensor): Latent conditioning vector with shape (..., latent_dim)\n",
    "\n",
    "        Returns:\n",
    "            x (tensor): Samples from target distribution with shape (*num_samples, dim)\n",
    "        \"\"\"\n",
    "                \n",
    "        if self.base_dist is None:\n",
    "            raise ValueError('Please define `base_dist` if you need sampling')\n",
    "        if isinstance(num_samples, int):\n",
    "            num_samples = (num_samples,)\n",
    "\n",
    "        x = self.base_dist.rsample(num_samples)\n",
    "        x, log_jac_diag = self.forward(x, latent, mask, **kwargs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f097c71-180e-4b7c-bcd4-5c7309de2bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exact_model(\n",
    "    dim,\n",
    "    hidden_dims,\n",
    "    latent_dim,\n",
    "    context_dim=0,\n",
    "    n_transforms=4,\n",
    "    n_heads=2,\n",
    "    model=\"deepset\",\n",
    "    set_data=False,\n",
    "    device=\"cpu\",\n",
    "    atol=1e-4,\n",
    "    base_dist_mean=None,\n",
    "    base_dist_cov=None,\n",
    "):\n",
    "    has_latent = True if context_dim > 0 else False\n",
    "\n",
    "    transforms = []\n",
    "\n",
    "    for _ in range(n_transforms):\n",
    "        if model == \"deepset\":\n",
    "            net = st.net.DiffeqExactTraceDeepSet(\n",
    "                dim, hidden_dims, dim, d_h=latent_dim, latent_dim=context_dim\n",
    "            )\n",
    "        elif model == \"settransformer\":\n",
    "            net = st.net.DiffeqExactTraceAttention(\n",
    "                dim,\n",
    "                hidden_dims,\n",
    "                dim,\n",
    "                d_h=latent_dim,\n",
    "                n_heads=n_heads,\n",
    "                latent_dim=context_dim,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        transforms.append(\n",
    "            st.flows.ContinuousTransform(\n",
    "                dim,\n",
    "                net=net,\n",
    "                divergence=\"exact\",\n",
    "                solver=\"dopri5\",\n",
    "                atol=atol,\n",
    "                has_latent=has_latent,\n",
    "                set_data=set_data,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if base_dist_mean is None:\n",
    "        base_dist_mean = torch.zeros(dim)\n",
    "\n",
    "    if base_dist_cov is None:\n",
    "        base_dist_cov = torch.ones(dim)\n",
    "\n",
    "    model = Flow(\n",
    "        st.Normal(base_dist_mean.to(device), base_dist_cov.to(device)), transforms\n",
    "    ).to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f4d2d6-7e5c-4946-ba4f-15594f98a8f2",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4e638d-a92a-4d75-afa7-d8378482f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_simulations = 1_000\n",
    "dim_theta = 4\n",
    "dim_x = 100\n",
    "\n",
    "theta = torch.random.randn(1_000, dim_theta)\n",
    "x = torch.random.randn(1_000, dim_x)\n",
    "dataset = TensorDataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c33084-6bab-4235-b982-b20e3b161888",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = get_exact_model(\n",
    "    dim=3,\n",
    "    hidden_dims=[64, 64],\n",
    "    latent_dim=8,\n",
    "    context_dim=0,\n",
    "    n_transforms=2,\n",
    "    n_heads=2,\n",
    "    model=\"deepset\",\n",
    "    set_data=True,\n",
    "    # base_dist_mean=x_mean,\n",
    "    # base_dist_cov=x_cov,\n",
    "    # device=device,\n",
    "    atol=1e-4,\n",
    ")\n",
    "\n",
    "x = torch.randn(10, 10, 3)\n",
    "print(network.log_prob(x, mask=torch.ones_like(x)), network.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e919d8c-5d4f-457f-8ed7-1d70fe69ef17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
