{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1a781b3-ac9a-43bd-97ff-bbb0ad6e5d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e485e76-bece-454d-82f1-e7b281a10b3e",
   "metadata": {},
   "source": [
    "# SBI Example with Permutation Invariant Density\n",
    "\n",
    "The code inspiration can be found [here](https://github.com/smsharma/jet-setting/blob/1c07c72f3354936093589f66547d2face89036f3/notebooks/01_jets_set_transformer.ipynb#L129).\n",
    "\n",
    "The idea is that we want to create a normalizing flow which is permutation invariant in the density for each of the peaks. That requires a special formulation of continuous normalizing flows, which can be found [here](https://arxiv.org/abs/2010.03242) and [here](https://arxiv.org/abs/2206.09021). The following implements those flows. We can sample from them and take the log prob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8556a53-7a99-41b6-bdce-0a1c704aa9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stribor as st\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(os.getcwd()).parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e0d6af-fccf-4670-89eb-7896d9f2456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flow(nn.Module):\n",
    "    \"\"\"\n",
    "    Building both normalizing flows and neural flows.\n",
    "\n",
    "    Example:\n",
    "    >>> import stribor as st\n",
    "    >>> torch.manual_seed(123)\n",
    "    >>> dim = 2\n",
    "    >>> flow = st.Flow(st.UnitNormal(dim), [st.Affine(dim)])\n",
    "    >>> x = torch.rand(1, dim)\n",
    "    >>> y, ljd = flow(x)\n",
    "    >>> y_inv, ljd_inv = flow.inverse(y)\n",
    "\n",
    "    Args:\n",
    "        base_dist (Type[torch.distributions]): Base distribution\n",
    "        transforms (List[st.flows]): List of invertible transformations\n",
    "    \"\"\"\n",
    "    def __init__(self, base_dist=None, transforms=[]):\n",
    "        super().__init__()\n",
    "        self.base_dist = base_dist\n",
    "        self.transforms = nn.ModuleList(transforms)\n",
    "\n",
    "    def forward(self, x, latent=None, mask=None, t=None, reverse=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (tensor): Input sampled from base density with shape (..., dim)\n",
    "            latent (tensor, optional): Conditional vector with shape (..., latent_dim)\n",
    "                Default: None\n",
    "            mask (tensor): Masking tensor with shape (..., 1)\n",
    "                Default: None\n",
    "            t (tensor, optional): Flow time end point. Default: None\n",
    "            reverse (bool, optional): Whether to perform an inverse. Default: False\n",
    "\n",
    "        Returns:\n",
    "            y (tensor): Output that follows target density (..., dim)\n",
    "            log_jac_diag (tensor): Log-Jacobian diagonal (..., dim)\n",
    "        \"\"\"\n",
    "        transforms = self.transforms[::-1] if reverse else self.transforms\n",
    "        _mask = 1 if mask is None else mask\n",
    "\n",
    "        log_jac_diag = torch.zeros_like(x).to(x)\n",
    "        for f in transforms:\n",
    "            if reverse:\n",
    "                x, ld = f.inverse_and_log_det_jacobian(x * _mask, latent=latent, mask=mask, t=t, **kwargs)\n",
    "            else:\n",
    "                x, ld = f.forward_and_log_det_jacobian(x * _mask, latent=latent, mask=mask, t=t, **kwargs)\n",
    "            log_jac_diag += ld * _mask\n",
    "        return x, log_jac_diag\n",
    "\n",
    "    def inverse(self, y, latent=None, mask=None, t=None, **kwargs):\n",
    "        \"\"\" Inverse of forward function with the same arguments. \"\"\"\n",
    "        return self.forward(y, latent=latent, mask=mask, t=t, reverse=True, **kwargs)\n",
    "\n",
    "    def log_prob(self, x, **kwargs):\n",
    "        \"\"\"\n",
    "        Calculates log-probability of a sample.\n",
    "\n",
    "        Args:\n",
    "            x (tensor): Input with shape (..., dim)\n",
    "\n",
    "        Returns:\n",
    "            log_prob (tensor): Log-probability of the input with shape (..., 1)\n",
    "        \"\"\"\n",
    "        if self.base_dist is None:\n",
    "            raise ValueError('Please define `base_dist` if you need log-probability')\n",
    "        x, log_jac_diag = self.inverse(x, **kwargs)\n",
    "\n",
    "        log_prob = self.base_dist.log_prob(x) + log_jac_diag.sum(-1)\n",
    "        return log_prob.unsqueeze(-1)\n",
    "\n",
    "    def sample(self, num_samples, latent=None, mask=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Transforms samples from the base to the target distribution.\n",
    "        Uses reparametrization trick.\n",
    "\n",
    "        Args:\n",
    "            num_samples (tuple or int): Shape of samples\n",
    "            latent (tensor): Latent conditioning vector with shape (..., latent_dim)\n",
    "\n",
    "        Returns:\n",
    "            x (tensor): Samples from target distribution with shape (*num_samples, dim)\n",
    "        \"\"\"\n",
    "                \n",
    "        if self.base_dist is None:\n",
    "            raise ValueError('Please define `base_dist` if you need sampling')\n",
    "        if isinstance(num_samples, int):\n",
    "            num_samples = (num_samples,)\n",
    "\n",
    "        x = self.base_dist.rsample(num_samples)\n",
    "        x, log_jac_diag = self.forward(x, latent, mask, **kwargs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f097c71-180e-4b7c-bcd4-5c7309de2bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exact_model(\n",
    "    dim,\n",
    "    hidden_dims,\n",
    "    latent_dim,\n",
    "    context_dim=0,\n",
    "    n_transforms=4,\n",
    "    n_heads=2,\n",
    "    model=\"deepset\",\n",
    "    set_data=False,\n",
    "    device=\"cpu\",\n",
    "    atol=1e-4,\n",
    "    base_dist_mean=None,\n",
    "    base_dist_cov=None,\n",
    "):\n",
    "    has_latent = True if context_dim > 0 else False\n",
    "\n",
    "    transforms = []\n",
    "\n",
    "    for _ in range(n_transforms):\n",
    "        if model == \"deepset\":\n",
    "            net = st.net.DiffeqExactTraceDeepSet(\n",
    "                dim, hidden_dims, dim, d_h=latent_dim, latent_dim=context_dim\n",
    "            )\n",
    "        elif model == \"settransformer\":\n",
    "            net = st.net.DiffeqExactTraceAttention(\n",
    "                dim,\n",
    "                hidden_dims,\n",
    "                dim,\n",
    "                d_h=latent_dim,\n",
    "                n_heads=n_heads,\n",
    "                latent_dim=context_dim,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        transforms.append(\n",
    "            st.flows.ContinuousTransform(\n",
    "                dim,\n",
    "                net=net,\n",
    "                divergence=\"exact\",\n",
    "                solver=\"dopri5\",\n",
    "                atol=atol,\n",
    "                has_latent=has_latent,\n",
    "                set_data=set_data,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if base_dist_mean is None:\n",
    "        base_dist_mean = torch.zeros(dim)\n",
    "\n",
    "    if base_dist_cov is None:\n",
    "        base_dist_cov = torch.ones(dim)\n",
    "\n",
    "    model = Flow(\n",
    "        st.Normal(base_dist_mean.to(device), base_dist_cov.to(device)), transforms\n",
    "    ).to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f4d2d6-7e5c-4946-ba4f-15594f98a8f2",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "\n",
    "We want to fit a density model $q_1(\\text{params} \\mid K, D) q_2(K \\mid D)$, in this case we're working on $q_1(\\text{params} \\mid K, D)$. This thing needs to be permutation invariant because relabeling parameters shouldn't matter. Secondly, it needs to accept a variable number of parameters (takes in K). The variable number of parameters part is giving me some trouble since we need a special data type for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3830590b-0e68-4734-8f19-a343e2721ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ycounts_all = torch.from_numpy(np.loadtxt(root / \"data/jumpingflows_simplified_bursts_data.dat\"))\n",
    "k_all = torch.from_numpy(np.loadtxt(root / \"data/jumpingflows_simplified_bursts_k.dat\"))\n",
    "# TODO: load the batch parameters which are of size [B, K_i (variable), num_parameters]\n",
    "\n",
    "num_simulations = ycounts_all.shape[0]\n",
    "assert num_simulations == k_all.shape[0]\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "# TODO: create the correct dataset for this. it needs to allow variable length data.\n",
    "dataset = TensorDataset(k_all, ycounts_all)\n",
    "train_dataset, valid_dataset = random_split(dataset, [80_000, 20_000])\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size,\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=batch_size, \n",
    ")\n",
    "\n",
    "# This network should enable log_prob and sampling (and therefore training) if you set the hyper parameter correctly\n",
    "network = get_exact_model(\n",
    "    dim=3,\n",
    "    hidden_dims=[64, 64],\n",
    "    latent_dim=8,\n",
    "    context_dim=0,\n",
    "    n_transforms=2,\n",
    "    n_heads=2,\n",
    "    model=\"deepset\",\n",
    "    set_data=True,\n",
    "    # base_dist_mean=x_mean,\n",
    "    # base_dist_cov=x_cov,\n",
    "    # device=device,\n",
    "    atol=1e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e919d8c-5d4f-457f-8ed7-1d70fe69ef17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
